{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbeb147d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-26T17:15:59.663951Z",
     "iopub.status.busy": "2025-03-26T17:15:59.663751Z",
     "iopub.status.idle": "2025-03-26T17:16:00.406515Z",
     "shell.execute_reply": "2025-03-26T17:16:00.405573Z"
    },
    "papermill": {
     "duration": 0.747934,
     "end_time": "2025-03-26T17:16:00.407914",
     "exception": false,
     "start_time": "2025-03-26T17:15:59.659980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /kaggle/input/chest-xray-pneumonia\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9df9df9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T17:16:00.414372Z",
     "iopub.status.busy": "2025-03-26T17:16:00.414131Z",
     "iopub.status.idle": "2025-03-26T17:16:01.004001Z",
     "shell.execute_reply": "2025-03-26T17:16:01.002913Z"
    },
    "papermill": {
     "duration": 0.594477,
     "end_time": "2025-03-26T17:16:01.005451",
     "exception": false,
     "start_time": "2025-03-26T17:16:00.410974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/local/bin/kaggle\", line 5, in <module>\r\n",
      "    from kaggle.cli import main\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 7, in <module>\r\n",
      "    api.authenticate()\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 407, in authenticate\r\n",
      "    raise IOError('Could not find {}. Make sure it\\'s located in'\r\n",
      "OSError: Could not find kaggle.json. Make sure it's located in /kaggle/working. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/kaggle/working\"\n",
    "\n",
    "# Verify Kaggle API\n",
    "!kaggle datasets list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3cf7096",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T17:16:01.012303Z",
     "iopub.status.busy": "2025-03-26T17:16:01.012056Z",
     "iopub.status.idle": "2025-03-26T17:19:15.575680Z",
     "shell.execute_reply": "2025-03-26T17:19:15.574491Z"
    },
    "papermill": {
     "duration": 194.568515,
     "end_time": "2025-03-26T17:19:15.576882",
     "exception": true,
     "start_time": "2025-03-26T17:16:01.008367",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL: 4023 samples (25.71%)\n",
      "PNEUMONIA: 11625 samples (74.29%)\n",
      "NORMAL: 234 samples (37.50%)\n",
      "PNEUMONIA: 390 samples (62.50%)\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94668760/94668760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m29084464/29084464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The filepath provided must end in `.keras` (Keras model format). Received: filepath=best_resnet_model.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6b42e27b56cc>\u001b[0m in \u001b[0;36m<cell line: 335>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;31m# Initialize and run pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPneumoniaClassificationPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m     \u001b[0mresnet_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensenet_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;31m# Example prediction (replace with actual X-ray image path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-6b42e27b56cc>\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;31m# Model checkpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         resnet_checkpoint = ModelCheckpoint(\n\u001b[0m\u001b[1;32m    270\u001b[0m             \u001b[0;34m'best_resnet_model.h5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, initial_value_threshold)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    192\u001b[0m                     \u001b[0;34m\"The filepath provided must end in `.keras` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                     \u001b[0;34m\"(Keras model format). Received: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The filepath provided must end in `.keras` (Keras model format). Received: filepath=best_resnet_model.h5"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "# Deep Learning and ML Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50V2, DenseNet121\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, GlobalAveragePooling2D, Input, \n",
    "    Dropout, Flatten, Concatenate\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Additional Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    classification_report, \n",
    "    roc_curve, \n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Memory Management and Augmentation\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "class PneumoniaClassificationPipeline:\n",
    "    def __init__(self, dataset_path):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline with dataset paths and configuration\n",
    "        \n",
    "        Args:\n",
    "            dataset_path (str): Root path of the chest X-ray dataset\n",
    "        \"\"\"\n",
    "        self.dataset_path = dataset_path\n",
    "        self.train_path = os.path.join(dataset_path, 'train')\n",
    "        self.test_path = os.path.join(dataset_path, 'test')\n",
    "        self.val_path = os.path.join(dataset_path, 'val')\n",
    "        \n",
    "        # Configuration parameters\n",
    "        self.img_height = 224\n",
    "        self.img_width = 224\n",
    "        self.batch_size = 16  # Reduced batch size for memory efficiency\n",
    "        self.epochs = 30  # Reduced epochs\n",
    "        \n",
    "    def load_and_preprocess_data(self, directory, is_test=False):\n",
    "        \"\"\"\n",
    "        Load and preprocess chest X-ray images with memory-efficient approach\n",
    "        \n",
    "        Args:\n",
    "            directory (str): Directory to load images from\n",
    "            is_test (bool): Whether this is test data (no augmentation)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Preprocessed image data and labels\n",
    "        \"\"\"\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        # Data augmentation for training\n",
    "        if not is_test:\n",
    "            datagen = ImageDataGenerator(\n",
    "                rotation_range=20,\n",
    "                width_shift_range=0.2,\n",
    "                height_shift_range=0.2,\n",
    "                horizontal_flip=True,\n",
    "                zoom_range=0.2,\n",
    "                shear_range=0.2,\n",
    "                fill_mode='nearest'\n",
    "            )\n",
    "        \n",
    "        for label in ['NORMAL', 'PNEUMONIA']:\n",
    "            class_path = os.path.join(directory, label)\n",
    "            class_label = 1 if label == 'PNEUMONIA' else 0\n",
    "            \n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                try:\n",
    "                    # Load and preprocess image\n",
    "                    img = load_img(img_path, target_size=(self.img_height, self.img_width))\n",
    "                    img_array = img_to_array(img) / 255.0  # Normalize\n",
    "                    \n",
    "                    # Apply augmentation only for training\n",
    "                    if not is_test:\n",
    "                        # Generate additional augmented images\n",
    "                        for _ in range(2):  # Generate 2 additional augmented images\n",
    "                            augmented = datagen.random_transform(img_array)\n",
    "                            images.append(augmented)\n",
    "                            labels.append(class_label)\n",
    "                    \n",
    "                    # Original image\n",
    "                    images.append(img_array)\n",
    "                    labels.append(class_label)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {img_path}: {e}\")\n",
    "        \n",
    "        return np.array(images), np.array(labels)\n",
    "    \n",
    "    def visualize_class_distribution(self, y, title):\n",
    "        \"\"\"\n",
    "        Visualize class distribution\n",
    "        \n",
    "        Args:\n",
    "            y (array): Labels\n",
    "            title (str): Plot title\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        plt.bar(['NORMAL', 'PNEUMONIA'], counts)\n",
    "        plt.title(title)\n",
    "        plt.ylabel('Number of Samples')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Print distribution details\n",
    "        for label, count in zip(['NORMAL', 'PNEUMONIA'], counts):\n",
    "            percentage = count / len(y) * 100\n",
    "            print(f\"{label}: {count} samples ({percentage:.2f}%)\")\n",
    "    \n",
    "    def build_resnet_model(self):\n",
    "        \"\"\"\n",
    "        Build ResNet50V2 model for pneumonia classification\n",
    "        \n",
    "        Returns:\n",
    "            Model: Compiled ResNet model\n",
    "        \"\"\"\n",
    "        # Base ResNet model\n",
    "        base_model = ResNet50V2(\n",
    "            weights='imagenet', \n",
    "            include_top=False, \n",
    "            input_shape=(self.img_height, self.img_width, 3)\n",
    "        )\n",
    "        \n",
    "        # Freeze base model layers\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        # Add custom layers\n",
    "        model = Sequential([\n",
    "            base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(512, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.0001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_densenet_model(self):\n",
    "        \"\"\"\n",
    "        Build DenseNet121 model for pneumonia classification\n",
    "        \n",
    "        Returns:\n",
    "            Model: Compiled DenseNet model\n",
    "        \"\"\"\n",
    "        # Base DenseNet model\n",
    "        base_model = DenseNet121(\n",
    "            weights='imagenet', \n",
    "            include_top=False, \n",
    "            input_shape=(self.img_height, self.img_width, 3)\n",
    "        )\n",
    "        \n",
    "        # Freeze base model layers\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        # Add custom layers\n",
    "        model = Sequential([\n",
    "            base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(512, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.0001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def evaluate_model(self, model, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Comprehensive model evaluation\n",
    "        \n",
    "        Args:\n",
    "            model (Model): Trained model\n",
    "            X_test (array): Test images\n",
    "            y_test (array): Test labels\n",
    "        \n",
    "        Returns:\n",
    "            dict: Detailed model performance metrics\n",
    "        \"\"\"\n",
    "        # Predict probabilities\n",
    "        y_pred_proba = model.predict(X_test).ravel()\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        \n",
    "        # Compute metrics\n",
    "        results = {\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "            'classification_report': classification_report(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "            'precision_recall_auc': average_precision_score(y_test, y_pred_proba)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def predict_pneumonia(self, model, image_path):\n",
    "        \"\"\"\n",
    "        Predict pneumonia for a single X-ray image\n",
    "        \n",
    "        Args:\n",
    "            model (Model): Trained model\n",
    "            image_path (str): Path to the X-ray image\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Prediction probability and label\n",
    "        \"\"\"\n",
    "        # Load and preprocess image\n",
    "        img = load_img(image_path, target_size=(self.img_height, self.img_width))\n",
    "        img_array = img_to_array(img) / 255.0\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        \n",
    "        # Predict\n",
    "        prediction_proba = model.predict(img_array)[0][0]\n",
    "        prediction_label = \"PNEUMONIA\" if prediction_proba > 0.5 else \"NORMAL\"\n",
    "        \n",
    "        return prediction_proba, prediction_label\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Run the complete machine learning pipeline\n",
    "        \"\"\"\n",
    "        # Load and preprocess training data\n",
    "        X_train, y_train = self.load_and_preprocess_data(self.train_path)\n",
    "        \n",
    "        # Visualize original training data distribution\n",
    "        self.visualize_class_distribution(y_train, \"Original Training Data Distribution\")\n",
    "        \n",
    "        # Load and preprocess test data\n",
    "        X_test, y_test = self.load_and_preprocess_data(self.test_path, is_test=True)\n",
    "        \n",
    "        # Visualize test data distribution\n",
    "        self.visualize_class_distribution(y_test, \"Test Data Distribution\")\n",
    "        \n",
    "        # Prepare models\n",
    "        resnet_model = self.build_resnet_model()\n",
    "        densenet_model = self.build_densenet_model()\n",
    "        \n",
    "        # Model checkpoints\n",
    "        resnet_checkpoint = ModelCheckpoint(\n",
    "            'best_resnet_model.h5', \n",
    "            monitor='val_accuracy', \n",
    "            save_best_only=True\n",
    "        )\n",
    "        densenet_checkpoint = ModelCheckpoint(\n",
    "            'best_densenet_model.h5', \n",
    "            monitor='val_accuracy', \n",
    "            save_best_only=True\n",
    "        )\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Train ResNet model\n",
    "        print(\"\\nTraining ResNet Model...\")\n",
    "        resnet_history = resnet_model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_split=0.2,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            callbacks=[resnet_checkpoint, early_stopping]\n",
    "        )\n",
    "        \n",
    "        # Train DenseNet model\n",
    "        print(\"\\nTraining DenseNet Model...\")\n",
    "        densenet_history = densenet_model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_split=0.2,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            callbacks=[densenet_checkpoint, early_stopping]\n",
    "        )\n",
    "        \n",
    "        # Evaluate models\n",
    "        print(\"\\nEvaluating ResNet Model:\")\n",
    "        resnet_results = self.evaluate_model(resnet_model, X_test, y_test)\n",
    "        print_model_results(resnet_results, \"ResNet\")\n",
    "        \n",
    "        print(\"\\nEvaluating DenseNet Model:\")\n",
    "        densenet_results = self.evaluate_model(densenet_model, X_test, y_test)\n",
    "        print_model_results(densenet_results, \"DenseNet\")\n",
    "        \n",
    "        return resnet_model, densenet_model\n",
    "\n",
    "def print_model_results(results, model_name):\n",
    "    \"\"\"\n",
    "    Print detailed model evaluation results\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Model performance metrics\n",
    "        model_name (str): Name of the model\n",
    "    \"\"\"\n",
    "    print(f\"\\n{model_name} Model Performance:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(results['confusion_matrix'])\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(results['classification_report'])\n",
    "    print(f\"ROC AUC: {results['roc_auc']:.4f}\")\n",
    "    print(f\"Precision-Recall AUC: {results['precision_recall_auc']:.4f}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Dataset path\n",
    "    dataset_path = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"\n",
    "    \n",
    "    # Initialize and run pipeline\n",
    "    pipeline = PneumoniaClassificationPipeline(dataset_path)\n",
    "    resnet_model, densenet_model = pipeline.run_pipeline()\n",
    "    \n",
    "    # Example prediction (replace with actual X-ray image path)\n",
    "    sample_pneumonia_image = \"/path/to/sample/pneumonia/image.jpg\"\n",
    "    sample_normal_image = \"/path/to/sample/normal/image.jpg\"\n",
    "    \n",
    "    # Predict using ResNet\n",
    "    print(\"\\nResNet Model Predictions:\")\n",
    "    resnet_pneumonia_pred = pipeline.predict_pneumonia(resnet_model, sample_pneumonia_image)\n",
    "    resnet_normal_pred = pipeline.predict_pneumonia(resnet_model, sample_normal_image)\n",
    "    print(f\"Pneumonia Image - Probability: {resnet_pneumonia_pred[0]:.4f}, Label: {resnet_pneumonia_pred[1]}\")\n",
    "    print(f\"Normal Image - Probability: {resnet_normal_pred[0]:.4f}, Label: {resnet_normal_pred[1]}\")\n",
    "    \n",
    "    # Predict using DenseNet\n",
    "    print(\"\\nDenseNet Model Predictions:\")\n",
    "    densenet_pneumonia_pred = pipeline.predict_pneumonia(densenet_model, sample_pneumonia_image)\n",
    "    densenet_normal_pred = pipeline.predict_pneumonia(densenet_model, sample_normal_image)\n",
    "    print(f\"Pneumonia Image - Probability: {densenet_pneumonia_pred[0]:.4f}, Label: {densenet_pneumonia_pred[1]}\")\n",
    "    print(f\"Normal Image - Probability: {densenet_normal_pred[0]:.4f}, Label: {densenet_normal_pred[1]}\")\n",
    "\n",
    "# Requirements (to be installed):\n",
    "# tensorflow\n",
    "# scikit-learn\n",
    "# matplotlib\n",
    "# opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa4298",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T16:36:03.281068Z",
     "iopub.status.busy": "2025-03-26T16:36:03.280632Z",
     "iopub.status.idle": "2025-03-26T16:36:03.338738Z",
     "shell.execute_reply": "2025-03-26T16:36:03.337598Z",
     "shell.execute_reply.started": "2025-03-26T16:36:03.281038Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "# Deep Learning and ML Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50V2, DenseNet121\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, GlobalAveragePooling2D, Input, \n",
    "    Dropout, Flatten, Concatenate\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Additional Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    classification_report, \n",
    "    roc_curve, \n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Memory Management and Augmentation\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "class PneumoniaClassificationPipeline:\n",
    "    def __init__(self, dataset_path):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline with dataset paths and configuration\n",
    "        \n",
    "        Args:\n",
    "            dataset_path (str): Root path of the chest X-ray dataset\n",
    "        \"\"\"\n",
    "        self.dataset_path = dataset_path\n",
    "        self.train_path = os.path.join(dataset_path, 'train')\n",
    "        self.test_path = os.path.join(dataset_path, 'test')\n",
    "        self.val_path = os.path.join(dataset_path, 'val')\n",
    "        \n",
    "        # Configuration parameters\n",
    "        self.img_height = 224\n",
    "        self.img_width = 224\n",
    "        self.batch_size = 16  # Reduced batch size for memory efficiency\n",
    "        self.epochs = 30  # Reduced epochs\n",
    "        \n",
    "    def load_and_preprocess_data(self, directory, is_test=False):\n",
    "        \"\"\"\n",
    "        Load and preprocess chest X-ray images with memory-efficient approach\n",
    "        \n",
    "        Args:\n",
    "            directory (str): Directory to load images from\n",
    "            is_test (bool): Whether this is test data (no augmentation)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Preprocessed image data and labels\n",
    "        \"\"\"\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        # Data augmentation for training\n",
    "        if not is_test:\n",
    "            datagen = ImageDataGenerator(\n",
    "                rotation_range=20,\n",
    "                width_shift_range=0.2,\n",
    "                height_shift_range=0.2,\n",
    "                horizontal_flip=True,\n",
    "                zoom_range=0.2,\n",
    "                shear_range=0.2,\n",
    "                fill_mode='nearest'\n",
    "            )\n",
    "        \n",
    "        for label in ['NORMAL', 'PNEUMONIA']:\n",
    "            class_path = os.path.join(directory, label)\n",
    "            class_label = 1 if label == 'PNEUMONIA' else 0\n",
    "            \n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                try:\n",
    "                    # Load and preprocess image\n",
    "                    img = load_img(img_path, target_size=(self.img_height, self.img_width))\n",
    "                    img_array = img_to_array(img) / 255.0  # Normalize\n",
    "                    \n",
    "                    # Apply augmentation only for training\n",
    "                    if not is_test:\n",
    "                        # Generate additional augmented images\n",
    "                        for _ in range(2):  # Generate 2 additional augmented images\n",
    "                            augmented = datagen.random_transform(img_array)\n",
    "                            images.append(augmented)\n",
    "                            labels.append(class_label)\n",
    "                    \n",
    "                    # Original image\n",
    "                    images.append(img_array)\n",
    "                    labels.append(class_label)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {img_path}: {e}\")\n",
    "        \n",
    "        return np.array(images), np.array(labels)\n",
    "    \n",
    "    def visualize_class_distribution(self, y, title):\n",
    "        \"\"\"\n",
    "        Visualize class distribution\n",
    "        \n",
    "        Args:\n",
    "            y (array): Labels\n",
    "            title (str): Plot title\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        plt.bar(['NORMAL', 'PNEUMONIA'], counts)\n",
    "        plt.title(title)\n",
    "        plt.ylabel('Number of Samples')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Print distribution details\n",
    "        for label, count in zip(['NORMAL', 'PNEUMONIA'], counts):\n",
    "            percentage = count / len(y) * 100\n",
    "            print(f\"{label}: {count} samples ({percentage:.2f}%)\")\n",
    "    \n",
    "    def build_resnet_model(self):\n",
    "        \"\"\"\n",
    "        Build ResNet50V2 model for pneumonia classification\n",
    "        \n",
    "        Returns:\n",
    "            Model: Compiled ResNet model\n",
    "        \"\"\"\n",
    "        # Base ResNet model\n",
    "        base_model = ResNet50V2(\n",
    "            weights='imagenet', \n",
    "            include_top=False, \n",
    "            input_shape=(self.img_height, self.img_width, 3)\n",
    "        )\n",
    "        \n",
    "        # Freeze base model layers\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        # Add custom layers\n",
    "        model = Sequential([\n",
    "            base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(512, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.0001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_densenet_model(self):\n",
    "        \"\"\"\n",
    "        Build DenseNet121 model for pneumonia classification\n",
    "        \n",
    "        Returns:\n",
    "            Model: Compiled DenseNet model\n",
    "        \"\"\"\n",
    "        # Base DenseNet model\n",
    "        base_model = DenseNet121(\n",
    "            weights='imagenet', \n",
    "            include_top=False, \n",
    "            input_shape=(self.img_height, self.img_width, 3)\n",
    "        )\n",
    "        \n",
    "        # Freeze base model layers\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        # Add custom layers\n",
    "        model = Sequential([\n",
    "            base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(512, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.0001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def evaluate_model(self, model, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Comprehensive model evaluation\n",
    "        \n",
    "        Args:\n",
    "            model (Model): Trained model\n",
    "            X_test (array): Test images\n",
    "            y_test (array): Test labels\n",
    "        \n",
    "        Returns:\n",
    "            dict: Detailed model performance metrics\n",
    "        \"\"\"\n",
    "        # Predict probabilities\n",
    "        y_pred_proba = model.predict(X_test).ravel()\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        \n",
    "        # Compute metrics\n",
    "        results = {\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "            'classification_report': classification_report(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "            'precision_recall_auc': average_precision_score(y_test, y_pred_proba)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def predict_pneumonia(self, model, image_path):\n",
    "        \"\"\"\n",
    "        Predict pneumonia for a single X-ray image\n",
    "        \n",
    "        Args:\n",
    "            model (Model): Trained model\n",
    "            image_path (str): Path to the X-ray image\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Prediction probability and label\n",
    "        \"\"\"\n",
    "        # Load and preprocess image\n",
    "        img = load_img(image_path, target_size=(self.img_height, self.img_width))\n",
    "        img_array = img_to_array(img) / 255.0\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        \n",
    "        # Predict\n",
    "        prediction_proba = model.predict(img_array)[0][0]\n",
    "        prediction_label = \"PNEUMONIA\" if prediction_proba > 0.5 else \"NORMAL\"\n",
    "        \n",
    "        return prediction_proba, prediction_label\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Run the complete machine learning pipeline\n",
    "        \"\"\"\n",
    "        # Load and preprocess training data\n",
    "        X_train, y_train = self.load_and_preprocess_data(self.train_path)\n",
    "        \n",
    "        # Visualize original training data distribution\n",
    "        self.visualize_class_distribution(y_train, \"Original Training Data Distribution\")\n",
    "        \n",
    "        # Load and preprocess test data\n",
    "        X_test, y_test = self.load_and_preprocess_data(self.test_path, is_test=True)\n",
    "        \n",
    "        # Visualize test data distribution\n",
    "        self.visualize_class_distribution(y_test, \"Test Data Distribution\")\n",
    "        \n",
    "        # Prepare models\n",
    "        resnet_model = self.build_resnet_model()\n",
    "        densenet_model = self.build_densenet_model()\n",
    "        \n",
    "        # Model checkpoints (updated to .keras extension)\n",
    "        resnet_checkpoint = ModelCheckpoint(\n",
    "            'best_resnet_model.keras', \n",
    "            monitor='val_accuracy', \n",
    "            save_best_only=True\n",
    "        )\n",
    "        densenet_checkpoint = ModelCheckpoint(\n",
    "            'best_densenet_model.keras', \n",
    "            monitor='val_accuracy', \n",
    "            save_best_only=True\n",
    "        )\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Train ResNet model\n",
    "        print(\"\\nTraining ResNet Model...\")\n",
    "        resnet_history = resnet_model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_split=0.2,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            callbacks=[resnet_checkpoint, early_stopping]\n",
    "        )\n",
    "        \n",
    "        # Train DenseNet model\n",
    "        print(\"\\nTraining DenseNet Model...\")\n",
    "        densenet_history = densenet_model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_split=0.2,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            callbacks=[densenet_checkpoint, early_stopping]\n",
    "        )\n",
    "        \n",
    "        # Evaluate models\n",
    "        print(\"\\nEvaluating ResNet Model:\")\n",
    "        resnet_results = self.evaluate_model(resnet_model, X_test, y_test)\n",
    "        print_model_results(resnet_results, \"ResNet\")\n",
    "        \n",
    "        print(\"\\nEvaluating DenseNet Model:\")\n",
    "        densenet_results = self.evaluate_model(densenet_model, X_test, y_test)\n",
    "        print_model_results(densenet_results, \"DenseNet\")\n",
    "        \n",
    "        return resnet_model, densenet_model\n",
    "\n",
    "def print_model_results(results, model_name):\n",
    "    \"\"\"\n",
    "    Print detailed model evaluation results\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Model performance metrics\n",
    "        model_name (str): Name of the model\n",
    "    \"\"\"\n",
    "    print(f\"\\n{model_name} Model Performance:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(results['confusion_matrix'])\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(results['classification_report'])\n",
    "    print(f\"ROC AUC: {results['roc_auc']:.4f}\")\n",
    "    print(f\"Precision-Recall AUC: {results['precision_recall_auc']:.4f}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Dataset path (you'll need to replace this with your actual path)\n",
    "    dataset_path = \"/path/to/chest_xray\"\n",
    "    \n",
    "    # Initialize and run pipeline\n",
    "    pipeline = PneumoniaClassificationPipeline(dataset_path)\n",
    "    resnet_model, densenet_model = pipeline.run_pipeline()\n",
    "    \n",
    "    # Example prediction (replace with actual X-ray image paths)\n",
    "    sample_pneumonia_image = \"/kaggle/input/chest-xray-pneumonia/chest_xray/test/PNEUMONIA/person100_bacteria_477.jpeg\"\n",
    "    sample_normal_image = \"/kaggle/input/chest-xray-pneumonia/chest_xray/test/NORMAL/IM-0006-0001.jpeg\"\n",
    "    \n",
    "    # Predict using ResNet\n",
    "    print(\"\\nResNet Model Predictions:\")\n",
    "    resnet_pneumonia_pred = pipeline.predict_pneumonia(resnet_model, sample_pneumonia_image)\n",
    "    resnet_normal_pred = pipeline.predict_pneumonia(resnet_model, sample_normal_image)\n",
    "    print(f\"Pneumonia Image - Probability: {resnet_pneumonia_pred[0]:.4f}, Label: {resnet_pneumonia_pred[1]}\")\n",
    "    print(f\"Normal Image - Probability: {resnet_normal_pred[0]:.4f}, Label: {resnet_normal_pred[1]}\")\n",
    "    \n",
    "    # Predict using DenseNet\n",
    "    print(\"\\nDenseNet Model Predictions:\")\n",
    "    densenet_pneumonia_pred = pipeline.predict_pneumonia(densenet_model, sample_pneumonia_image)\n",
    "    densenet_normal_pred = pipeline.predict_pneumonia(densenet_model, sample_normal_image)\n",
    "    print(f\"Pneumonia Image - Probability: {densenet_pneumonia_pred[0]:.4f}, Label: {densenet_pneumonia_pred[1]}\")\n",
    "    print(f\"Normal Image - Probability: {densenet_normal_pred[0]:.4f}, Label: {densenet_normal_pred[1]}\")\n",
    "\n",
    "# Requirements (to be installed):\n",
    "# tensorflow\n",
    "# scikit-learn\n",
    "# matplotlib\n",
    "# opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db24c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T16:49:00.804972Z",
     "iopub.status.busy": "2025-03-26T16:49:00.804570Z",
     "iopub.status.idle": "2025-03-26T16:57:37.584871Z",
     "shell.execute_reply": "2025-03-26T16:57:37.583827Z",
     "shell.execute_reply.started": "2025-03-26T16:49:00.804945Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "# Deep Learning and ML Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50V2, DenseNet121\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, GlobalAveragePooling2D, Input, \n",
    "    Dropout, Flatten, Conv2D, \n",
    "    MaxPooling2D, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Additional Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    classification_report, \n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# Imbalance Handling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# SHAP for Interpretability\n",
    "import shap\n",
    "\n",
    "class AdvancedPneumoniaClassificationPipeline:\n",
    "    def __init__(self, dataset_path):\n",
    "        \"\"\"\n",
    "        Initialize the advanced pipeline with memory-efficient configurations\n",
    "        \n",
    "        Args:\n",
    "            dataset_path (str): Root path of the chest X-ray dataset\n",
    "        \"\"\"\n",
    "        self.dataset_path = dataset_path\n",
    "        self.train_path = os.path.join(dataset_path, 'train')\n",
    "        self.test_path = os.path.join(dataset_path, 'test')\n",
    "        \n",
    "        # Reduced configuration parameters to save memory\n",
    "        self.img_height = 224\n",
    "        self.img_width = 224\n",
    "        self.batch_size = 16  # Reduced batch size\n",
    "        self.epochs = 30  # Reduced epochs\n",
    "        self.num_classes = 2\n",
    "        \n",
    "        # Configure GPU memory growth\n",
    "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            try:\n",
    "                for gpu in gpus:\n",
    "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            except RuntimeError as e:\n",
    "                print(e)\n",
    "    \n",
    "    def load_and_preprocess_images(self, directory, is_test=False, max_images=None):\n",
    "        \"\"\"\n",
    "        Memory-efficient image loading and preprocessing\n",
    "        \n",
    "        Args:\n",
    "            directory (str): Directory to load images from\n",
    "            is_test (bool): Whether this is test data (no augmentation)\n",
    "            max_images (int, optional): Limit number of images to load\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Preprocessed image data and labels\n",
    "        \"\"\"\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        # Simplified data augmentation\n",
    "        if not is_test:\n",
    "            datagen = ImageDataGenerator(\n",
    "                rotation_range=20,\n",
    "                width_shift_range=0.1,\n",
    "                height_shift_range=0.1,\n",
    "                horizontal_flip=True,\n",
    "                zoom_range=0.1\n",
    "            )\n",
    "        \n",
    "        label_map = {'NORMAL': 0, 'PNEUMONIA': 1}\n",
    "        \n",
    "        for label, class_index in label_map.items():\n",
    "            class_path = os.path.join(directory, label)\n",
    "            image_files = os.listdir(class_path)\n",
    "            \n",
    "            # Limit images if specified\n",
    "            if max_images:\n",
    "                image_files = image_files[:max_images]\n",
    "            \n",
    "            for img_name in image_files:\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                try:\n",
    "                    # More memory-efficient image loading\n",
    "                    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    img = cv2.resize(img, (self.img_height, self.img_width))\n",
    "                    img = img / 255.0  # Normalize\n",
    "                    \n",
    "                    # Apply augmentation only for training\n",
    "                    if not is_test:\n",
    "                        # Generate fewer augmented images\n",
    "                        for _ in range(1):  # Generate 1 additional augmented image\n",
    "                            augmented = datagen.random_transform(img)\n",
    "                            images.append(augmented)\n",
    "                            labels.append(class_index)\n",
    "                    \n",
    "                    # Original image\n",
    "                    images.append(img)\n",
    "                    labels.append(class_index)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {img_path}: {e}\")\n",
    "        \n",
    "        return np.array(images), np.array(labels)\n",
    "    \n",
    "    def apply_smote(self, X, y):\n",
    "        \"\"\"\n",
    "        Apply SMOTE and Tomek Links for balanced dataset with memory efficiency\n",
    "        \n",
    "        Args:\n",
    "            X (array): Input features\n",
    "            y (array): Input labels\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Balanced dataset\n",
    "        \"\"\"\n",
    "        # Flatten the image for SMOTE\n",
    "        X_flat = X.reshape(X.shape[0], -1)\n",
    "        \n",
    "        # Create SMOTE-Tomek Links pipeline\n",
    "        smote_tomek = ImbPipeline([\n",
    "            ('smote', SMOTE(sampling_strategy='auto', random_state=42)),\n",
    "            ('tomek', TomekLinks())\n",
    "        ])\n",
    "        \n",
    "        # Apply SMOTE-Tomek\n",
    "        X_resampled, y_resampled = smote_tomek.fit_resample(X_flat, y)\n",
    "        \n",
    "        # Reshape back to image dimensions\n",
    "        X_resampled = X_resampled.reshape(\n",
    "            X_resampled.shape[0], \n",
    "            self.img_height, \n",
    "            self.img_width, \n",
    "            3\n",
    "        )\n",
    "        \n",
    "        return X_resampled, y_resampled\n",
    "    \n",
    "    def build_ensemble_models(self):\n",
    "        \"\"\"\n",
    "        Build multiple deep learning models for ensemble with memory-efficient design\n",
    "        \n",
    "        Returns:\n",
    "            list: Compiled models\n",
    "        \"\"\"\n",
    "        # ResNet50V2 Model\n",
    "        resnet_base = ResNet50V2(\n",
    "            weights='imagenet', \n",
    "            include_top=False, \n",
    "            input_shape=(self.img_height, self.img_width, 3)\n",
    "        )\n",
    "        resnet_base.trainable = False\n",
    "        \n",
    "        resnet_model = Sequential([\n",
    "            resnet_base,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(256, activation='relu'),  # Reduced dense layer size\n",
    "            Dropout(0.3),  # Adjusted dropout\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        # DenseNet121 Model\n",
    "        densenet_base = DenseNet121(\n",
    "            weights='imagenet', \n",
    "            include_top=False, \n",
    "            input_shape=(self.img_height, self.img_width, 3)\n",
    "        )\n",
    "        densenet_base.trainable = False\n",
    "        \n",
    "        densenet_model = Sequential([\n",
    "            densenet_base,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dense(256, activation='relu'),  # Reduced dense layer size\n",
    "            Dropout(0.3),  # Adjusted dropout\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        # Custom Lightweight CNN Model\n",
    "        inputs = Input(shape=(self.img_height, self.img_width, 3))\n",
    "        x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D()(x)\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D()(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        lightweight_model = Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        # Compile models\n",
    "        models = [resnet_model, densenet_model, lightweight_model]\n",
    "        \n",
    "        for model in models:\n",
    "            model.compile(\n",
    "                optimizer=Adam(learning_rate=0.0001),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "        \n",
    "        return models\n",
    "    \n",
    "    def ensemble_predict(self, models, X):\n",
    "        \"\"\"\n",
    "        Ensemble prediction using soft voting\n",
    "        \n",
    "        Args:\n",
    "            models (list): List of trained models\n",
    "            X (array): Input images\n",
    "        \n",
    "        Returns:\n",
    "            array: Ensemble predictions\n",
    "        \"\"\"\n",
    "        predictions = [model.predict(X, verbose=0) for model in models]\n",
    "        ensemble_pred = np.mean(predictions, axis=0)\n",
    "        return ensemble_pred\n",
    "    \n",
    "    def run_advanced_pipeline(self):\n",
    "        \"\"\"\n",
    "        Run the complete advanced machine learning pipeline with memory optimization\n",
    "        \"\"\"\n",
    "        # Load and preprocess training data with image limit\n",
    "        X_train, y_train = self.load_and_preprocess_images(self.train_path, max_images=1000)\n",
    "        \n",
    "        # Visualize original training data distribution\n",
    "        self.visualize_class_distribution(y_train, \"Original Training Data Distribution\", 'before')\n",
    "        \n",
    "        # Apply SMOTE to balance dataset\n",
    "        X_train_resampled, y_train_resampled = self.apply_smote(X_train, y_train)\n",
    "        \n",
    "        # Visualize balanced training data distribution\n",
    "        self.visualize_class_distribution(y_train_resampled, \"Balanced Training Data Distribution\", 'after')\n",
    "        \n",
    "        # Load test data\n",
    "        X_test, y_test = self.load_and_preprocess_images(self.test_path, is_test=True, max_images=500)\n",
    "        \n",
    "        # Prepare ensemble models\n",
    "        models = self.build_ensemble_models()\n",
    "        \n",
    "        # Train models with early stopping and model checkpointing\n",
    "        trained_models = []\n",
    "        for i, model in enumerate(models, 1):\n",
    "            print(f\"\\nTraining Model {i}\")\n",
    "            checkpoint = ModelCheckpoint(\n",
    "                f'best_model_{i}.keras', \n",
    "                monitor='val_accuracy', \n",
    "                save_best_only=True\n",
    "            )\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss', \n",
    "                patience=5, \n",
    "                restore_best_weights=True\n",
    "            )\n",
    "            \n",
    "            history = model.fit(\n",
    "                X_train_resampled, y_train_resampled,\n",
    "                validation_split=0.2,\n",
    "                epochs=self.epochs,\n",
    "                batch_size=self.batch_size,\n",
    "                callbacks=[checkpoint, early_stopping],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            trained_models.append(model)\n",
    "        \n",
    "        # Ensemble prediction\n",
    "        ensemble_pred_proba = self.ensemble_predict(trained_models, X_test)\n",
    "        ensemble_pred = (ensemble_pred_proba > 0.5).astype(int)\n",
    "        \n",
    "        # Evaluate ensemble performance\n",
    "        print(\"\\nEnsemble Model Performance:\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_test, ensemble_pred))\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, ensemble_pred))\n",
    "        print(f\"ROC AUC: {roc_auc_score(y_test, ensemble_pred_proba):.4f}\")\n",
    "        \n",
    "        return trained_models, ensemble_pred_proba\n",
    "    \n",
    "    def visualize_class_distribution(self, y, title, before_after='before'):\n",
    "        \"\"\"\n",
    "        Visualize class distribution with enhanced plotting\n",
    "        \n",
    "        Args:\n",
    "            y (array): Labels\n",
    "            title (str): Plot title\n",
    "            before_after (str): Indicate before or after SMOTE\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        plt.bar(['NORMAL', 'PNEUMONIA'], counts)\n",
    "        plt.title(f'{title} - {before_after.capitalize()} SMOTE')\n",
    "        plt.ylabel('Number of Samples')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{title.lower().replace(\" \", \"_\")}_{before_after}_smote.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Print distribution details\n",
    "        for label, count in zip(['NORMAL', 'PNEUMONIA'], counts):\n",
    "            percentage = count / len(y) * 100\n",
    "            print(f\"{label}: {count} samples ({percentage:.2f}%)\")\n",
    "    \n",
    "    def predict_pneumonia(self, models, image_path):\n",
    "        \"\"\"\n",
    "        Predict pneumonia for a single X-ray image using ensemble\n",
    "        \n",
    "        Args:\n",
    "            models (list): Trained models\n",
    "            image_path (str): Path to the X-ray image\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Prediction probability and label\n",
    "        \"\"\"\n",
    "        # Load and preprocess image\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (self.img_height, self.img_width))\n",
    "        img = img / 255.0\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        \n",
    "        # Ensemble prediction\n",
    "        prediction_proba = self.ensemble_predict(models, img)[0][0]\n",
    "        prediction_label = \"PNEUMONIA\" if prediction_proba > 0.5 else \"NORMAL\"\n",
    "        \n",
    "        return prediction_proba, prediction_label\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Updated dataset path\n",
    "    dataset_path = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"\n",
    "    \n",
    "    # Initialize and run advanced pipeline\n",
    "    pipeline = AdvancedPneumoniaClassificationPipeline(dataset_path)\n",
    "    ensemble_models, predictions = pipeline.run_advanced_pipeline()\n",
    "    \n",
    "    # Example predictions\n",
    "    sample_pneumonia_image = os.path.join(dataset_path, \"test/PNEUMONIA/person100_bacteria_477.jpeg\")\n",
    "    sample_normal_image = os.path.join(dataset_path, \"test/NORMAL/IM-0006-0001.jpeg\")\n",
    "    \n",
    "    # Predict using ensemble\n",
    "    print(\"\\nEnsemble Model Predictions:\")\n",
    "    pneumonia_pred = pipeline.predict_pneumonia(ensemble_models, sample_pneumonia_image)\n",
    "    normal_pred = pipeline.predict_pneumonia(ensemble_models, sample_normal_image)\n",
    "    \n",
    "    print(f\"Pneumonia Image - Probability: {pneumonia_pred[0]:.4f}, Label: {pneumonia_pred[1]}\")\n",
    "    print(f\"Normal Image - Probability: {normal_pred[0]:.4f}, Label: {normal_pred[1]}\")\n",
    "\n",
    "# Requirements (to be installed):\n",
    "# tensorflow\n",
    "# scikit-learn\n",
    "# imbalanced-learn\n",
    "# matplotlib\n",
    "# opencv-python\n",
    "# shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42973236",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 23851,
     "datasetId": 17810,
     "sourceId": 23812,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 11577198,
     "datasetId": 6975243,
     "sourceId": 11175933,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 201.86172,
   "end_time": "2025-03-26T17:19:18.908685",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-26T17:15:57.046965",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
